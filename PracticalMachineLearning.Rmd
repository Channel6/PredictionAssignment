---
title: "Practical Machine Learning"
author: "Glenn Kerbein"
date: "May 2, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify *how well they do it*.

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

# Data Processing
Load relevant libraries
```{r echo=TRUE}
suppressMessages(library(caret))
suppressMessages(library(randomForest))
suppressMessages(library(corrplot))
suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
suppressMessages(library(RColorBrewer))
suppressMessages(library(rattle))
suppressMessages(library(e1071))
```

# Download the Data
```{r echo=TRUE}
trainingdatafile <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testdatafile  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("pml-testing.csv")){
	download.file(testdatafile, "pml-testing.csv", method="curl")
}
if(!file.exists("pml-training.csv")){
	download.file(trainingdatafile, "pml-training.csv", method="curl")
}
```

# Read the Data
After downloading the relevant CSV files, we now read them into relevant variables.
```{r echo=TRUE}
training  <- read.csv(trainingdatafile)
testing <- read.csv(testdatafile)
```
# Exploring the data
```{r echo=TRUE}
dim(training)
dim(testing)
names(training)
```
The training data set contains 19622 observations and 160 variables, while the testing data set contains 20 observations and 160 variables. The "classe" variable in the training set is the outcome to predict.

# Clean the data
Set any foreign characters to NA first.
Remove the variable(s) that countain most NA values. These are 'X', ‘user_name’ and several timestamp-related variables, represented by culumns 1-5.
```{r echo=TRUE}
training[training == "#DIV/0!"]  <- NA
training[training == ""]  <- NA

training <- training[, colSums(is.na(training)) == 0] 
testing <- testing[, colSums(is.na(testing)) == 0] 

training <- training[,-c(1,5)]
testing <- testing[,-c(1,5)]

testing  <- testing[, colSums(is.na(testing)) == 0]
training  <- training[, colSums(is.na(training)) == 0]
```
# Create a cross validation set from the training data set
```{r echo=TRUE}
set.seed(90011009)

inTrain <- createDataPartition(training$classe, p=3/4, list=FALSE)
trainingdata <- training[inTrain, ]
testingdata <- training[-inTrain, ]
```

# Creating the model using Random Forest method
Using the ```trainControl``` function, then passing this to ```train()``` will perform this task.
## Fitting Random Forest.
Random Forest has a high accurancy rate, combined with robust correlated covariates and outlier selection. This is the most desirable.
Cross validation is used as train control method with 5-fold as parameter number. For best fold seleciton, ```tuneRF``` would find the optimal value.
```{r echo=TRUE}
rfcontrol <- trainControl(
	method="cv",
	5
)
rfmodel <- train(
	classe ~ .,
	data=trainingdata,
	method="rf",
	trControl=rfcontrol,
	ntree=250
)
rfmodel
```
# Performance evaluation of the Random forest algorithm.
Estimate the performance of the model on the validation data set (our ```testingdata``` variable). A ```confusionMatrix()``` call, presented below, as well as the estimated accuracy and the the estimated out-of-sample error of the model are calculated.
```{r echo=TRUE}
rfpredict <- predict(rfmodel, testingdata)
confusionMatrix(testingdata$classe, rfpredict)
```
```{r echo=TRUE}
accuracy <- postResample(rfpredict, testingdata$classe)
accuracy
```
```{r echo=TRUE}
accuracy <- postResample(rfpredict, testingdata$classe)
accuracy
```
# Prediction for the Test Data Set
```{r echo=TRUE}
final_result <- predict(rfmodel, testingdata)
levels(final_result)
```

# Appendix
## Correlation Matrix
Below is a visual Correlation Matrix of ```trainingdata```, which is the 3/4 data from the split of ```training```. ```coorPlot()```, from the coorPlot package, creates this graphic.
```training``` must be numeric so ```trainingdatanumber``` can be created with ```trainingdatanumber <- trainingdata[, sapply(trainingdata, is.numeric)]```.
```{r echo=TRUE}
corrPlot <- cor(trainingdatanumber[, -length(names(trainingdatanumber))])
corrplot(corrPlot, method="color")
```

## Decision Tree Visualization of trainData using rpart
```{r echo=TRUE}
tree_tree <- rpart(classe ~ ., data=trainingdata, method="class")
prp(tree_tree)
```
